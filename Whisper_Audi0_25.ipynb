{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnBwd6i53NESXUxwiImbUF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-Monaghan/whisper-audio/blob/main/Whisper_Audi0_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9FCcfY5IyQ8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whisper AI: Advanced Audio Transcription, Translation, and Analysis\n",
        "This Google Colaboratory notebook provides a comprehensive environment for leveraging OpenAI's Whisper model for various audio processing tasks. It goes beyond basic transcription to include features like language detection, translation, speaker diarization, and text summarization.\n",
        "\n",
        "Features:\n",
        "\n",
        "Easy Setup: Install all necessary libraries with a single cell.\n",
        "\n",
        "Flexible Audio Input: Upload files or use files from Google Drive.\n",
        "\n",
        "Whisper Model Selection: Choose from various Whisper models (tiny, base, small, medium, large).\n",
        "\n",
        "Transcription: Convert audio to text with automatic language detection.\n",
        "\n",
        "Translation: Translate transcribed text to English.\n",
        "\n",
        "Speaker Diarization: Identify and label different speakers in the audio (requires Hugging Face token).\n",
        "\n",
        "Text Summarization: Generate concise summaries of the transcribed content.\n",
        "\n",
        "Structured Output: Save results in various formats (TXT, SRT, VTT, JSON).\n",
        "\n",
        "1. Setup and Installation\n",
        "This section will install all the necessary libraries, including whisper, ffmpeg (for audio processing), pyannote.audio (for speaker diarization), and transformers (for summarization)."
      ],
      "metadata": {
        "id": "7dANVvOaIy-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install Dependencies\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q ffmpeg-python\n",
        "!pip install -q transformers\n",
        "!pip install -q accelerate\n",
        "!pip install -q sentencepiece # Required for some tokenizer models\n",
        "!pip install -q pydub # For audio manipulation\n",
        "\n",
        "# Install pyannote.audio for speaker diarization (requires specific versions)\n",
        "# Note: pyannote.audio requires a Hugging Face token for models.\n",
        "!pip install -q pyannote.audio==3.1.1\n",
        "!pip install -q torchaudio==2.0.2\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torch Audio version: {torchaudio.__version__}\")\n",
        "\n",
        "import whisper\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "from transformers import pipeline\n",
        "import json\n",
        "import re\n",
        "from IPython.display import Audio, display\n",
        "import numpy as np\n",
        "import subprocess\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"All dependencies installed and imported successfully!\")"
      ],
      "metadata": {
        "id": "bh55_ICYI0UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Google Drive Integration (Optional)\n",
        "Mount your Google Drive to easily access audio files and save output files."
      ],
      "metadata": {
        "id": "N7LEKfMAI4C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted!\")\n"
      ],
      "metadata": {
        "id": "LG6VhC5YI561"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Audio Input Handling\n",
        "You can either upload an audio file directly to Colab or specify a path to an audio file already in your Google Drive. This section also includes a utility to convert audio to a suitable format if needed."
      ],
      "metadata": {
        "id": "5Y3wBB5oI7_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload Audio File or Specify Path\n",
        "# @markdown Choose your audio input method:\n",
        "audio_input_method = \"Upload File\"  # @param [\"Upload File\", \"Google Drive Path\"]\n",
        "\n",
        "# @markdown If \"Upload File\" is selected, use the upload button that appears after running this cell.\n",
        "# @markdown If \"Google Drive Path\" is selected, provide the full path to your audio file (e.g., `/content/drive/MyDrive/my_audio.mp3`).\n",
        "google_drive_audio_path = \"/content/drive/MyDrive/my_audio.mp3\" # @param {type:\"string\"}\n",
        "\n",
        "audio_file_path = None\n",
        "\n",
        "if audio_input_method == \"Upload File\":\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        audio_file_path = filename\n",
        "        print(f\"Uploaded file: {audio_file_path}\")\n",
        "        break # Assuming only one file is uploaded\n",
        "else:\n",
        "    if os.path.exists(google_drive_audio_path):\n",
        "        audio_file_path = google_drive_audio_path\n",
        "        print(f\"Using Google Drive file: {audio_file_path}\")\n",
        "    else:\n",
        "        print(f\"Error: File not found at {google_drive_audio_path}. Please check the path.\")\n",
        "\n",
        "# Function to convert audio to WAV (Whisper prefers WAV)\n",
        "def convert_audio_to_wav(input_path, output_path=\"temp_audio.wav\"):\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(input_path)\n",
        "        audio.export(output_path, format=\"wav\")\n",
        "        print(f\"Converted '{input_path}' to '{output_path}'\")\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting audio: {e}\")\n",
        "        return None\n",
        "\n",
        "if audio_file_path:\n",
        "    # Convert to WAV if not already WAV\n",
        "    if not audio_file_path.lower().endswith(\".wav\"):\n",
        "        audio_file_path = convert_audio_to_wav(audio_file_path)\n",
        "        if not audio_file_path:\n",
        "            print(\"Audio conversion failed. Please ensure the input file is valid.\")\n",
        "    else:\n",
        "        print(f\"Audio file '{audio_file_path}' is already in WAV format.\")\n",
        "\n",
        "    # Display audio for verification\n",
        "    if audio_file_path and os.path.exists(audio_file_path):\n",
        "        display(Audio(audio_file_path))\n",
        "    else:\n",
        "        print(\"No valid audio file to display.\")"
      ],
      "metadata": {
        "id": "CgV53-h5I_6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Whisper Model Loading\n",
        "Choose the Whisper model size. Larger models provide better accuracy but require more computational resources and time.\n",
        "\n"
      ],
      "metadata": {
        "id": "iu1qfCkiJCID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Select Whisper Model\n",
        "whisper_model_size = \"base\" # @param [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\"]\n",
        "\n",
        "print(f\"Loading Whisper model: {whisper_model_size}...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "whisper_model = whisper.load_model(whisper_model_size, device=device)\n",
        "print(f\"Whisper model '{whisper_model_size}' loaded on {device} device.\")\n"
      ],
      "metadata": {
        "id": "6fMIQQb3JClG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Core Functionality\n",
        "This section contains the main functions for transcription, translation, speaker diarization, and summarization.\n",
        "\n",
        "5.1. Audio Transcription\n",
        "Transcribe the audio file using the loaded Whisper model. It automatically detects the language."
      ],
      "metadata": {
        "id": "idIaEarbJEPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transcribe Audio\n",
        "# @markdown Run this cell to transcribe the uploaded/selected audio file.\n",
        "# @markdown You can choose to save the output in different formats.\n",
        "output_formats = [\"txt\", \"srt\", \"vtt\", \"json\"] # @param [\"txt\", \"srt\", \"vtt\", \"json\"] {allow-input: true}\n",
        "output_filename_prefix = \"transcription\" # @param {type:\"string\"}\n",
        "\n",
        "def transcribe_audio(audio_path, model, output_formats=[\"txt\"]):\n",
        "    if not audio_path or not os.path.exists(audio_path):\n",
        "        print(\"Error: Audio file not found for transcription.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Starting transcription for {audio_path}...\")\n",
        "    result = model.transcribe(audio_path, verbose=True) # verbose=True shows progress\n",
        "    text = result[\"text\"]\n",
        "    language = result[\"language\"]\n",
        "    segments = result[\"segments\"]\n",
        "\n",
        "    print(f\"\\nTranscription complete. Detected language: {language}\")\n",
        "    print(\"\\n--- Full Transcription ---\")\n",
        "    print(text)\n",
        "    print(\"--------------------------\")\n",
        "\n",
        "    base_output_name = os.path.join(os.path.dirname(audio_path) if os.path.dirname(audio_path) else \".\", output_filename_prefix)\n",
        "\n",
        "    # Save outputs in specified formats\n",
        "    for fmt in output_formats:\n",
        "        output_path = f\"{base_output_name}.{fmt}\"\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            if fmt == \"txt\":\n",
        "                f.write(text)\n",
        "            elif fmt == \"srt\":\n",
        "                for i, segment in enumerate(segments):\n",
        "                    start_time = str(0) + str(int(segment['start'] // 3600)).zfill(2) + ':' + str(int((segment['start'] % 3600) // 60)).zfill(2) + ':' + str(int(segment['start'] % 60)).zfill(2) + ',' + str(int((segment['start'] * 1000) % 1000)).zfill(3)\n",
        "                    end_time = str(0) + str(int(segment['end'] // 3600)).zfill(2) + ':' + str(int((segment['end'] % 3600) // 60)).zfill(2) + ':' + str(int(segment['end'] % 60)).zfill(2) + ',' + str(int((segment['end'] * 1000) % 1000)).zfill(3)\n",
        "                    f.write(f\"{i + 1}\\n\")\n",
        "                    f.write(f\"{start_time} --> {end_time}\\n\")\n",
        "                    f.write(f\"{segment['text'].strip()}\\n\\n\")\n",
        "            elif fmt == \"vtt\":\n",
        "                f.write(\"WEBVTT\\n\\n\")\n",
        "                for segment in segments:\n",
        "                    start_time = str(int(segment['start'] // 3600)).zfill(2) + ':' + str(int((segment['start'] % 3600) // 60)).zfill(2) + ':' + str(int(segment['start'] % 60)).zfill(2) + '.' + str(int((segment['start'] * 1000) % 1000)).zfill(3)\n",
        "                    end_time = str(int(segment['end'] // 3600)).zfill(2) + ':' + str(int((segment['end'] % 3600) // 60)).zfill(2) + ':' + str(int(segment['end'] % 60)).zfill(2) + '.' + str(int((segment['end'] * 1000) % 1000)).zfill(3)\n",
        "                    f.write(f\"{start_time} --> {end_time}\\n\")\n",
        "                    f.write(f\"{segment['text'].strip()}\\n\\n\")\n",
        "            elif fmt == \"json\":\n",
        "                json.dump(result, f, indent=4)\n",
        "        print(f\"Transcription saved to {output_path}\")\n",
        "\n",
        "    return text, language, segments\n",
        "\n",
        "# Perform transcription\n",
        "transcribed_text, detected_language, audio_segments = transcribe_audio(audio_file_path, whisper_model, output_formats)\n"
      ],
      "metadata": {
        "id": "d2lIzYYsJIAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2. Text Translation\n",
        "Translate the transcribed text to English. Whisper's transcribe function can also directly translate to English if task=\"translate\" is specified."
      ],
      "metadata": {
        "id": "Yez8DuVxJJkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Translate Transcription to English\n",
        "# @markdown Run this cell to translate the previously transcribed text to English.\n",
        "# @markdown This uses Whisper's built-in translation capability.\n",
        "\n",
        "def translate_text_with_whisper(audio_path, model):\n",
        "    if not audio_path or not os.path.exists(audio_path):\n",
        "        print(\"Error: Audio file not found for translation.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Starting translation for {audio_path}...\")\n",
        "    # Use task=\"translate\" to translate to English\n",
        "    translation_result = model.transcribe(audio_path, task=\"translate\", verbose=True)\n",
        "    translated_text = translation_result[\"text\"]\n",
        "\n",
        "    print(\"\\n--- Translated Text (English) ---\")\n",
        "    print(translated_text)\n",
        "    print(\"---------------------------------\")\n",
        "\n",
        "    # Save translated text\n",
        "    translation_output_path = os.path.join(os.path.dirname(audio_file_path) if os.path.dirname(audio_file_path) else \".\", f\"{output_filename_prefix}_translated.txt\")\n",
        "    with open(translation_output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(translated_text)\n",
        "    print(f\"Translated text saved to {translation_output_path}\")\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "if audio_file_path and transcribed_text:\n",
        "    translated_content = translate_text_with_whisper(audio_file_path, whisper_model)\n",
        "else:\n",
        "    print(\"No audio file or transcription available for translation. Please run previous cells.\")\n"
      ],
      "metadata": {
        "id": "To_bWg92JMWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.3. Speaker Diarization\n",
        "Identify different speakers in the audio and associate them with their respective transcribed segments. This requires a Hugging Face token.\n",
        "\n",
        "Important: You need to accept the user conditions for the pyannote/speaker-diarization model on Hugging Face and provide your Hugging Face token.\n",
        "\n",
        "Go to https://huggingface.co/pyannote/speaker-diarization and accept the user conditions.\n",
        "\n",
        "Go to https://huggingface.co/settings/tokens and generate a new token with 'read' access.\n",
        "\n",
        "Paste your token in the cell below."
      ],
      "metadata": {
        "id": "EwVfFeE9JOI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Perform Speaker Diarization\n",
        "# @markdown **Important:** Enter your Hugging Face token below.\n",
        "# @markdown Get your token from: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "hugging_face_token = \"\" # @param {type:\"string\"}\n",
        "\n",
        "if not hugging_face_token:\n",
        "    print(\"Please provide your Hugging Face token to perform speaker diarization.\")\n",
        "else:\n",
        "    os.environ[\"HF_TOKEN\"] = hugging_face_token\n",
        "    try:\n",
        "        from pyannote.audio import Pipeline\n",
        "        diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=hugging_face_token)\n",
        "        diarization_pipeline.to(torch.device(device))\n",
        "        print(\"Speaker diarization pipeline loaded.\")\n",
        "\n",
        "        def diarize_audio(audio_path, segments):\n",
        "            if not audio_path or not os.path.exists(audio_path):\n",
        "                print(\"Error: Audio file not found for diarization.\")\n",
        "                return []\n",
        "\n",
        "            print(f\"Starting speaker diarization for {audio_path}...\")\n",
        "            # Diarize the audio file\n",
        "            diarization_result = diarization_pipeline(audio_path)\n",
        "\n",
        "            diarized_segments = []\n",
        "            for segment in segments:\n",
        "                start_time = segment['start']\n",
        "                end_time = segment['end']\n",
        "                text = segment['text']\n",
        "                speaker = \"UNKNOWN\" # Default speaker\n",
        "\n",
        "                # Find the speaker for this segment\n",
        "                for turn, _, speaker_label in diarization_result.itertracks(yield_labelling=True):\n",
        "                    # Check for overlap between transcription segment and diarization turn\n",
        "                    if max(start_time, turn.start) < min(end_time, turn.end):\n",
        "                        speaker = speaker_label\n",
        "                        break # Found a speaker for this segment\n",
        "\n",
        "                diarized_segments.append({\n",
        "                    \"start\": start_time,\n",
        "                    \"end\": end_time,\n",
        "                    \"text\": text,\n",
        "                    \"speaker\": speaker\n",
        "                })\n",
        "\n",
        "            print(\"\\n--- Diarized Transcription ---\")\n",
        "            for seg in diarized_segments:\n",
        "                print(f\"[ {seg['start']:.1f}s - {seg['end']:.1f}s ] Speaker {seg['speaker']}: {seg['text'].strip()}\")\n",
        "            print(\"------------------------------\")\n",
        "\n",
        "            # Save diarized output\n",
        "            diarized_output_path = os.path.join(os.path.dirname(audio_file_path) if os.path.dirname(audio_file_path) else \".\", f\"{output_filename_prefix}_diarized.txt\")\n",
        "            with open(diarized_output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                for seg in diarized_segments:\n",
        "                    f.write(f\"[ {seg['start']:.1f}s - {seg['end']:.1f}s ] Speaker {seg['speaker']}: {seg['text'].strip()}\\n\")\n",
        "            print(f\"Diarized transcription saved to {diarized_output_path}\")\n",
        "\n",
        "            return diarized_segments\n",
        "\n",
        "        if audio_file_path and audio_segments:\n",
        "            diarized_audio_data = diarize_audio(audio_file_path, audio_segments)\n",
        "        else:\n",
        "            print(\"No audio file or segments available for diarization. Please run previous cells.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or running diarization pipeline: {e}\")\n",
        "        print(\"Please ensure your Hugging Face token is correct and you have accepted the model's user conditions.\")\n"
      ],
      "metadata": {
        "id": "7YkNKprrJQtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.4. Text Summarization\n",
        "Summarize the transcribed text using a pre-trained summarization model from the Hugging Face transformers library."
      ],
      "metadata": {
        "id": "nhdAz_p4JTIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Summarize Transcribed Text\n",
        "# @markdown Run this cell to generate a summary of the transcribed text.\n",
        "# @markdown You can adjust the `max_length` and `min_length` for the summary.\n",
        "summary_max_length = 150 # @param {type:\"integer\"}\n",
        "summary_min_length = 50 # @param {type:\"integer\"}\n",
        "\n",
        "def summarize_text(text, max_length=150, min_length=50):\n",
        "    if not text:\n",
        "        print(\"No text available for summarization.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Starting summarization (max_length={max_length}, min_length={min_length})...\")\n",
        "    try:\n",
        "        # Using a general-purpose summarization model\n",
        "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if torch.cuda.is_available() else -1)\n",
        "        summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
        "\n",
        "        print(\"\\n--- Summary ---\")\n",
        "        print(summary)\n",
        "        print(\"---------------\")\n",
        "\n",
        "        # Save summary\n",
        "        summary_output_path = os.path.join(os.path.dirname(audio_file_path) if os.path.dirname(audio_file_path) else \".\", f\"{output_filename_prefix}_summary.txt\")\n",
        "        with open(summary_output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(summary)\n",
        "        print(f\"Summary saved to {summary_output_path}\")\n",
        "\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(f\"Error during summarization: {e}\")\n",
        "        print(\"Ensure you have a stable internet connection for model download and sufficient memory.\")\n",
        "        return None\n",
        "\n",
        "if transcribed_text:\n",
        "    summarized_content = summarize_text(transcribed_text, summary_max_length, summary_min_length)\n",
        "else:\n",
        "    print(\"No transcribed text available for summarization. Please run the transcription cell first.\")\n"
      ],
      "metadata": {
        "id": "7T0S7A-SJVW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. Conclusion and Next Steps\n",
        "You have successfully used Whisper for advanced audio processing tasks!\n",
        "\n",
        "What's next?\n",
        "\n",
        "Experiment with Models: Try different Whisper model sizes to see the trade-off between speed and accuracy.\n",
        "\n",
        "Refine Diarization: For better diarization results, consider fine-tuning pyannote.audio or using more specialized models if available.\n",
        "\n",
        "Customization: Modify the output formats or integrate this script into a larger application.\n",
        "\n",
        "Batch Processing: Extend the script to process multiple audio files in a directory.\n",
        "\n",
        "UI Integration: Build a simple web interface (e.g., using Gradio or Streamlit) to make it more user-friendly."
      ],
      "metadata": {
        "id": "4vvJA2RnJY6B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}